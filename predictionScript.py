import torch
import torch.nn.functional as F
from torch.utils.data import DataLoader
import argparse
import os
import warnings
from datetime import datetime
from pathlib import Path
from tqdm import tqdm

# --- Local Imports ---
# Make sure model.py and utils.py are in the same directory or accessible
# You MUST use the modified "utils.py" I provided that crashes on error.
from model import GOSwinRaw
from utils import AudioLogSpecDataset, collate_fn_pad, OCSoftmaxLoss

# --- Custom Dataset for Prediction ---
class TestPredictionDataset(AudioLogSpecDataset):
    """
    Custom dataset for prediction. It loads data using the parent class
    but returns the audio_path_rel instead of the label.
    """
    def __init__(self, json_file_path, dataset_root_dir, max_audio_len=80000):
        super().__init__(json_file_path, dataset_root_dir, max_audio_len)
        print("[TestPredictionDataset] Initialized. Will return (wav, mask, spec, audio_path_rel)")

    def __getitem__(self, idx):
        # The metadata from a test_env.json might only have 3 items
        # or it might have dummy labels. Handle both.
        if len(self.metadata[idx]) >= 3:
             audio_path_rel, _, _ = self.metadata[idx][:3]
        else:
             audio_path_rel = self.metadata[idx][0] # Fallback if only path is present
        
        try:
            # super().__getitem__(idx) will return (waveform, mask, spec, label)
            # We ignore the label it returns, as it's a dummy 0 or -1
            waveform_padded, attention_mask, log_spec_cropped, _label = super().__getitem__(idx)
            
            # We return the audio_path_rel we got from the metadata
            return waveform_padded, attention_mask, log_spec_cropped, audio_path_rel
        except Exception as e:
            print(f"Error loading index {idx} ({audio_path_rel}): {e}. Returning None.")
            # This 'except' is OK because the 'super().__getitem__' will raise
            # the error first, thanks to our modified utils.py
            return None, None, None, None

def collate_fn_predict(batch):
    """
    Custom collate_fn that filters out None values from failed loads.
    """
    # Filter out samples that failed to load (returned as (None, None, None, None))
    valid_samples = [item for item in batch if item[0] is not None]
    
    if not valid_samples:
        return None, None, None, None # Return None if the whole batch failed

    raw_waveforms = [item[0] for item in valid_samples]
    attention_masks = [item[1] for item in valid_samples]
    spec_tensors = [item[2] for item in valid_samples]
    audio_paths = [item[3] for item in valid_samples] # This holds the file paths

    raw_waveforms_padded = torch.stack(raw_waveforms)
    attention_masks_padded = torch.stack(attention_masks)
    spec_tensors_padded = torch.stack(spec_tensors)
    
    return raw_waveforms_padded, attention_masks_padded, spec_tensors_padded, audio_paths

def run_prediction():
    parser = argparse.ArgumentParser(description="Run prediction on the test set and generate submission.txt")
    
    parser.add_argument('--model_path', type=str, required=True, help='Path to LOAD model checkpoint (.pth)')
    parser.add_argument('--test_json', type=str, required=True, help='Path to test_env.json (generated by preprocess_test_env.py)')
    parser.add_argument('--data_root', type=str, required=True, help='Root directory where audio/specs are stored (e.g., the folder containing "test_env_mels" and "test")')
    parser.add_argument('--output_file', type=str, default='submission.txt', help='Name of the output submission file (default: submission.txt)')
    parser.add_argument('--batch_size', type=int, default=16, help='Batch size for prediction')

    args = parser.parse_args()

    # --- SETUP ---
    warnings.filterwarnings("ignore", category=UserWarning)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # --- FIXED EER THRESHOLD ---
    # This should be the EER threshold you found on your VALIDATION set
    # The value 0.65 is just a placeholder. Use your actual validation EER threshold.
    EER_THRESHOLD = 0.65 
    
    print(f"Using device: {device}")
    print(f"--- IMPORTANT ---")
    print(f"Using FIXED Validation EER Threshold: {EER_THRESHOLD}")
    print(f"Applying logic: final_score = raw_score - {EER_THRESHOLD}")
    print(f"(Real: ~0.9 -> Positive | Fake: ~0.2 -> Negative)")
    print(f"-----------------")


    # --- Load Checkpoint ---
    if not os.path.exists(args.model_path):
        print(f"Error: Model checkpoint not found at {args.model_path}")
        return
    
    print(f"Loading checkpoint from {args.model_path}...")
    
    # Set weights_only=False because the checkpoint contains an argparse.Namespace
    checkpoint = torch.load(args.model_path, map_location=device, weights_only=False)
    
    # Load model args from checkpoint
    model_args = checkpoint.get('args', None)
    if model_args is None:
        print("Warning: Checkpoint does not contain 'args'. Using default model parameters.")
        embed_dim, raw_out_dim, swin_out_dim = 256, 256, 256
    else:
        if isinstance(model_args, argparse.Namespace):
            embed_dim = model_args.embed_dim
            raw_out_dim = model_args.raw_out_dim
            swin_out_dim = model_args.swin_out_dim
        else:
            print("Checkpoint 'args' is not a Namespace, attempting to access as dict.")
            try:
                embed_dim = model_args['embed_dim']
                raw_out_dim = model_args['raw_out_dim']
                swin_out_dim = model_args['swin_out_dim']
            except (TypeError, KeyError):
                print("Could not parse 'args' from checkpoint. Using default model parameters.")
                embed_dim, raw_out_dim, swin_out_dim = 256, 256, 256


    # --- Load Model ---
    model = GOSwinRaw(
        raw_out_dim=raw_out_dim,
        swin_out_dim=swin_out_dim,
        embed_dim=embed_dim
    ).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    print("Model loaded successfully.")
    
    # --- Load Loss Function (for the target weight) ---
    criterion = OCSoftmaxLoss(
        in_features=embed_dim,
    ).to(device)
    
    if 'criterion_state_dict' not in checkpoint:
        print("Error: Checkpoint does not contain 'criterion_state_dict'. Cannot compute scores.")
        return
        
    criterion.load_state_dict(checkpoint['criterion_state_dict'])
    criterion.eval()
    
    # Get the target weight vector (this is what defines "real" vs "fake")
    target_weight = torch.nn.functional.normalize(criterion.weight.data, p=2, dim=0)
    print("Trained criterion (target weight) loaded.")

    # --- Dataloader ---
    # We use the modified TestPredictionDataset
    test_dataset = TestPredictionDataset(args.test_json, args.data_root)
    test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, 
                             num_workers=4, collate_fn=collate_fn_predict, pin_memory=True)
                             
    # --- Prediction Loop ---
    print(f"Starting prediction... Output will be saved to {args.output_file}")
    print("--- SCORES (Filename, Score) ---")
    
    results = []
    with torch.no_grad():
        for (raw_padded, raw_mask, spec_padded, audio_paths) in tqdm(test_loader, desc="Predicting"):
            
            # Check if the whole batch failed to load
            if raw_padded is None:
                print("Warning: A full batch failed to load. Skipping.")
                continue
                
            raw_padded = raw_padded.to(device)
            raw_mask = raw_mask.to(device)
            spec_padded = spec_padded.to(device)
            
            embeddings = model(raw_padded, raw_mask, spec_padded)
            
            # Compute CM scores (cosine similarity to target weight)
            embeddings_norm = F.normalize(embeddings, p=2, dim=1)
            # This is the score where ~0.9=real, ~0.2=fake
            raw_scores = embeddings_norm.matmul(target_weight).cpu().numpy()
            
            # --- APPLY CORRECTED THRESHOLD LOGIC ---
            # final_score = raw_score - threshold
            # Real: 0.9 - 0.65 = +0.25 (Positive)
            # Fake: 0.2 - 0.65 = -0.45 (Negative)
            final_scores = raw_scores - EER_THRESHOLD
            # --- END ---
            
            # Store results
            for i in range(len(final_scores)):
                file_id = Path(audio_paths[i]).name
                score = final_scores[i]
                
                # --- ADDED PRINT STATEMENT FOR DEBUGGING ---
                print(f"{file_id},{score:.4f}")
                
                results.append(f"{file_id},{score:.4f}\n") # Format to 4 decimal places

    # --- Save Results ---
    with open(args.output_file, 'w') as f_out:
        f_out.writelines(results)
        
    print(f"----------------------------------")
    print(f"âœ… Prediction complete. {len(results)} scores saved to {args.output_file}.")

if __name__ == '__main__':
    run_prediction()